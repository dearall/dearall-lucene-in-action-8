## 4.7 域的多样性分析 Field variations ##

由于文档包含多个域的事实，使它具有多样的特性，这就给分析过程带来了一些有趣的要求。


<br/><br/>
<a id="1"></a>
## 4.7.1 分析多值域 Analysis of multivalued fields ##

回顾第 2 章，一个 Lucene 索引文档 Document 对象，可以包含具有相同名称的多个 Field 实例，而且，在索引时 Lucene 在逻辑上将这些域的词元按顺序追加在一起。幸运的是，分析器可以在每个域值的边界处提供一些控制。这对于确保查询关注到词元的位置是很重要的，例如短语或跨度查询，不会意外地匹配跨越两个不同的 Field 实例。例如，一个域值是 "it’s time to pay
income tax"，下一个域值是 "return library books on time"，那么对于短语搜索 "tax return" 则会匹配到这个域。

&emsp;&emsp;为了解决这个问题，应该通过子类化 Analyzer 类来创建自己的分析器，然后重写其 **getPositionIncrementGap(String fieldName)** 方法。默认情况下，getPositionIncrementGap(String fieldName) 返回 0 值（没有空隙），意思是后续的域值直接追加到前一个域值之后。把这个值增加到一个足够大的数值（例如 100），这样就不会有按位置的查询能在跨越域值边界进行错误的匹配了。

&emsp;&emsp;确保对多值域词元偏移量计算正确也很重要。如果要打算对这些域进行高亮显示，错误的偏移量会导致错误的文本部分被高亮。词元的 OffsetAttribute 特性，提供了检索起始和结束偏移量的方法，也提供了一个特殊的 endOffset() 方法，该方法的目的是返回域的最终偏移量，是比对应词元最后一个字符在原始文档中位置大于 1 的值。这对于词元过滤器剥离一个或多个词元的情况是必要的，否则，Lucene 就没有其他途径来计算该域值的最终偏移量。每一个 Field 实例的最终偏移量，是移动它之前的所有域的 endOffset 之和。Lucene 的核心分词器都正确地实现了 endOffset() 方法，但如果我们要创建自己的分词器，那么就得由我们自己来实现 endOffset() 方法了。类似地，如果我们的程序要求在某个域有多个域值时，给偏移量添加一个空隙，那么就应该重写自定义分析器的 getOffsetGap() 方法。



<br/><br/>
<a id="2"></a>
## 4.7.2 域特定的分析 Field-specific analysis ##

在索引期间，分析器选择的粒度是在 IndexWriter 级别，通过 IndexWriterConfig 对象的构造器配置。使用 QueryParser, 只有一个分析器应用于所有遇到的文本。然而，对于很多应用程序来说，文档包含各种各样的域，看起来每个域应使用不同的分析。

&emsp;&emsp;在内部，分析器能很容易地作用于要分析域的名称，因为域的名称作为参数传递给 Analyzer 的 tokenStream() 方法：

- **tokenStream(String fieldName, Reader reader)** 对从 Reader 型的 reader 读取的文本内容进行分词分析，返回适于 fieldName 域的 TokenStream。
- **tokenStream(String fieldName, String text)** 对 String 型的 text 文本进行分词分析，返回适于 fieldName 域的 TokenStream。

对于内置的分析器，没有利用这种针对于特定域进行分析的能力，因为它们的设计是为了通用目的的视野，并且域名称 fieldName 是特定于应用程序的，不过，我们可以很容易地创建自定义分析器，来利用针对于特定域进行分析的能力。另外，Lucene 提供了一个非常有用的工具类 PerFieldAnalyzerWrapper，位于 analyzers-common 模块的 org.apache.lucene.analysis.miscellaneous 包，可以很容易对每个域使用不同的分析器。它有两个构造器：

- **PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer)** 使用默认的分析器构造
- **PerFieldAnalyzerWrapper(Analyzer defaultAnalyzer, Map<String,Analyzer> fieldAnalyzers)** 使用默认的分析器以及一个域名和它使用的分析器 Map 进行构造


用法如下：

```
 Map<String,Analyzer> analyzerPerField = new HashMap<>();
 analyzerPerField.put("firstname", new KeywordAnalyzer());
 analyzerPerField.put("lastname", new KeywordAnalyzer());

 PerFieldAnalyzerWrapper aWrapper =
   new PerFieldAnalyzerWrapper(new StandardAnalyzer(EnglishAnalyzer.ENGLISH_STOP_WORDS_SET), analyzerPerField);
```

创建 PerFieldAnalyzerWrapper 实例时，我们提供了默认的分析器 StandardAnalyzer，并为不同的域提供了自己对应的分析器，通过一个 HashMap 传递给构造器。在索引时，任何没有指定特定分析器的域，简单地使用提供的默认分析器进行分析。对于上面的示例，除了 "firstname" 域和 "lastname" 域，都使用 StandardAnalyzer 进行分析。而 "firstname" 域和 "lastname" 域则使用已配置的自己对应的分析器进行分析。



<br/><br/>
<a id="3"></a>
## 4.7.3 搜索不进行分析的域 Searching on unanalyzed fields ##

有很多常见的场景更愿意不对内容进行分析而索引域的值。例如，部分数字，URL，社会安全代码（Social Security numbers）都应该直接进行索引，并且作为一个单独的词项进行搜索。在索引时，可以在创建域 Field 实例时，通过调用 FieldType 的	setTokenized(true) 很容易做到。我们也想让用户可以在这些固定的词项上直接进行搜索，在搜索程序上直接创建 TermQuery 就可以简单实现。

&emsp;&emsp;但是如果使用 QueryParser 并试图在没有进行分析的域上进行查询，就会面临窘境，因为事实是，只有索引期间知道域没有被分析。域的词项一旦被索引，就对它一无所知了，它们只是词项。通过一个直接的测试用例，看看对域不进行分析索引文档，然后试图再查询出该文档所暴露出来的问题。如代码清单 4.7.1 所示：

代码位于本书代码 analysis 子模块测试代码 net.mvnindex.demo.lucene.analysis.keyword 包。

<table width="100%"><tr><td bgcolor=green><font color=black>Listing 4.7.1 使用 QueryParser 匹配部分数字</td></tr></table>

```
public class KeywordAnalyzerTest {
  private final String indexPath = "indexes";
  private Directory directory;
  private DirectoryReader reader;
  private IndexSearcher searcher;

  @Before
  public void setUp() throws Exception {
    directory = FSDirectory.open(Paths.get(indexPath));
    IndexWriterConfig config = new IndexWriterConfig(new SimpleAnalyzer());
    IndexWriter writer = new IndexWriter(directory, config);

    Document doc = new Document();
    doc.add(new StringField("partnum",  // ①
                      "Q36",
                      Field.Store.NO));

    doc.add(new TextField("description",
                      "Illidium Space Modulator",
                      Field.Store.YES));

    writer.addDocument(doc);
    writer.close();

    reader = DirectoryReader.open(directory);
    searcher = new IndexSearcher(reader);
  }

  @After
  public void tearDown() throws Exception {
    reader.close();
    directory.close();
    deleteDir(new File(indexPath));
  }

  public static void deleteDir(File dir) {
    if (dir.isDirectory()) {
      String[] children = dir.list();
      for (int i = 0; i < children.length; i++) {
        new File(dir, children[i]).delete();
      }
    }
    dir.delete();
  }

  @Test
  public void testTermQuery() throws Exception {
    Query query = new TermQuery(new Term("partnum", "Q36"));    // ②
    assertEquals(1, TestUtil.hitCount(searcher, query));        // ③
  }

  @Test
  public void testBasicQueryParser() throws Exception {
    Query query = new QueryParser("description", new SimpleAnalyzer())          // ④
                      .parse("partnum:Q36 AND SPACE");                          // ④
    assertEquals("note Q36 -> q",
                 "+partnum:q +space", query.toString("description"));           // ⑤
    assertEquals("doc not found :(", 0, TestUtil.hitCount(searcher, query));

    System.out.println("query 表示： " + query.toString("description"));
  }
}

```

① StringField 不对域进行分析，通过 TYPE_NOT_STORED.setTokenized(true) 设置。
② 通过 API 直接创建 TermQuery，不对词项进行分析
③ 验证文档匹配
④ QueryParser 对查询表达式的每个词项和短语进行分析。Q36 和 SPACE 都被分析，并被切分为两个独立的词元。SimpleAnalyzer 剥离非字母字符并将词元字符转换为小写形式，因此 Q36 变为 q，而在索引时，Q36 是不进行分析的，保持原样编入索引。虽然索引时使用 SimpleAnalyzer 对 IndexWriter 进行了配置，但对于 "partnum" 域不起作用。
⑤ toString() 方法，输出 "+partnum:q +space"。Query 类有一个好用的 toString() 方法，返回查询类似于 QueryParser 的表达式，注意，Q36 经分析后，消失了。

&emsp;&emsp;TermQuery 工作得很好，但 QueryParser 没有找到结果。QueryParser 遇到了没有进行分析的域指出了问题的关键点：索引和分析都与搜索紧密关联。testBasicQueryParser() 测试展示了使用查询表达式经过分析之后，搜索通过 setTokenized(true) 创建的 term 会有问题的。问题的原因是，QueryParser 分析了 "partnum" 域，而该域是不应该进行分析的。这里有几个可能的解决方案：
- 修改应用程序 UI，让用户从自由形式的 QueryParser 之外选择一个 "partnum"。通常来说，最终用户不想知道（而且也无需知道）索引中域的名字。这种方法，容易实现，但并不建议使用，因为给最终用户提供一个以上的输入框是非常差的实践：用户可能变得非常困惑。
- 如果 "partnum" 或其它文本构造，在我们的分析的文本中经常出现的语汇，可以考虑创建一个自定义的领域特定分析器，能识别和保留它们。
- 继承 QueryParser 并重写它的一个或两个重载的 getFieldQuery() 方法，以提供某个域特定的处理。
- 使用 PerFieldAnalyzerWrapper 对域进行特定分析。

所有这些方案，最简单的就是使用 PerFieldAnalyzerWrapper 对域进行特定分析。

&emsp;&emsp;我们将使用 Lucene 的 KeywordAnalyzer 将 "partnum" 域分词为单一的词元。注意，KeywordAnalyzer 和 FieldType.setTokenized(true) 在索引期间具有相同的效果，只是对 QueryParser 使用 KeywordAnalyzer 是必要的。这种方式，我们只想对一个域进行分析，因此只把 PerFieldAnalyzerWrapper 应用到 "partnum" 域。首先，看一下通过 KeywordAnalyzer 实战来解决这种情况。如代码清单 4.7.2 所示：


<table width="100%"><tr><td bgcolor=green><font color=black>Listing 4.7.2 使用 PerFieldAnalyzerWrapper 配置 QueryParser 匹配部分数字</td></tr></table>

```
  @Test
  public void testPerFieldAnalyzer() throws Exception {
    Map<String, Analyzer> analyzerPerField = new HashMap<>();
    analyzerPerField.put("partnum", new KeywordAnalyzer());

    PerFieldAnalyzerWrapper analyzer = new PerFieldAnalyzerWrapper(new SimpleAnalyzer(),
            analyzerPerField);

    Query query = new QueryParser("description", analyzer).parse(
                "partnum:Q36 AND SPACE");

    assertEquals("Q36 kept as-is",
              "+partnum:Q36 +space", query.toString("description"));  
    assertEquals("doc found!", 1, TestUtil.hitCount(searcher, query));
  }
```

这里利用 PerFieldAnalyzerWrapper 将 KeywordAnalyzer 只应用到了 "partnum" 域，而 SimpleAnalyzer 应用到其它的域。这会产生与索引时相同的结果。现在的查询有了对 "partnum" 域正确的词项，并且文档也如期找到。

&emsp;&emsp;有了 KeywordAnalyzer 分析器，可以把代码进一步优化（在 KeywordAnalyzerTest 的 setUp() 方法中），并把用在 testPerFieldAnalyzer() 中相同的 PerFieldAnalyzerWrapper 用在索引期间。在索引时将 KeywordAnalyzer 分析器用在特定域上，可以消除 FieldType.setTokenized(true) 的调用。不过，现在已经有了 StringField 类的定义，这种变化也没什么意义。从审美的角度来说，在索引期间和查询期间使用相同的分析器是令人愉快的，而使用 PerFieldAnalyzerWrapper 使其称为可能。

&emsp;&emsp;已经看到了由于不同类型的域而引起的一些有趣的情况。多值域要求设置位置增量间隙，以避免匹配跨不同域值的情况，而 PerFieldAnalyzerWrapper 让我们能够自定义为域设置使用不同的分析器。

