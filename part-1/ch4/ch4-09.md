## 4.9 中文分析 ##

上一节讨论了 Lucene 分析关于不同语言方面的支持，以及针对非英语语种进行分析时需要面对的问题，并对 Lucene 自带的三个有中文处理能力的的分析器进行了测试和比较。可以看出，这三个自带的分析器 StandardAnalyzer、CJKAnalyzer、SmartChineseAnalyzer 都有分析中文文本的能力，而且能力一个比一个强，以 SmartChineseAnalyzer 能力最为强大。但它们的处理效果还是差强人意，并不能满足生产级搜索程序的需求。因此，通常我们会采用第三方开发的针对中文语系的分析器来处理中文内容。

本节介绍几个知名的可用于生产级应用程序的第三方中文分析器，并对它们的特性和配置方法进行深入的探讨。

<br/><br/>
<a id="1"></a>

## 4.9.1 中文分析器 IKAnalyzer ##

IKAnalyzer 是业内知名且流行的开源中文分析器，作者林良益 [linliangyi2007@gmail.com](mailto:linliangyi2007@gmail.com) 最初 2006 年 12 月发布 1.0 版，并在 GoogleCode: [http://code.google.com/p/ik-analyzer](http://code.google.com/p/ik-analyzer) 上开源。至 2012 年，IKAnalyzer 推出了 4 个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。从 3.0 版本开始，IKAnalyzer 发展为面向 Java 的公用分词组件，独立于 Lucene 项目，同时提供了对 Lucene 的默认优化实现。在 2012 版本中，IKAnalyzer 实现了简单的分词歧义排除算法，标志着 IKAnalyzer 分词器从单纯的词典分词向模拟语义分词衍化。


IKAnalyzer 2012 特性：
- 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和智能分词两种切分模式。
- 在系统环境：Core2 i7 3.4G双核，4G内存，window 7 64位， Sun JDK 1.6_29 64位 普通pc环境测试，IK2012具有160万字/秒（3000KB/S）的高速处理能力。
- 2012版本的智能分词模式支持简单的分词排歧义处理和数量词合并输出。
- 采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符。
- 优化的词典存储，更小的内存占用。支持用户词典扩展定义。特别的，在2012版本，词典支持中文，英文，数字混合词语。


2012 版之后，IKAnalyzer 已停止更新，github 上有很多克隆改造版本，用于适应新版本的 Lucene 以及 Solr 和 elasticsearch 使用环境。本小结内容克隆自 blueshen 的 github 版本 [https://github.com/blueshen/ik-analyzer.git](https://github.com/blueshen/ik-analyzer.git)，并为支持 Lucene 8.0 及其以上版本打了标签，方便使用。代码位于：

&emsp;&emsp;**[https://github.com/dearall/ik-analyzer.git](https://github.com/dearall/ik-analyzer.git)**

IKAnalyzer 有三个构造器：
- **IKAnalyzer()** 默认实现使用最细粒度切分算法
- **IKAnalyzer(boolean useSmart)** useSmart 为 true 时，分词器采用智能切分；为 false 时，分词器进行最细粒度切分。
- **IKAnalyzer(Configuration cfg)** 使用自定义的详细配置选项对分析器进行配置。


IKAnalyzer 自带测试代码如代码清单 4.9.1 所示：

<table width="100%"><tr><td bgcolor=green><font color=black>Listing 4.9.1 IKAnalzyerTest：</td></tr></table>

```java
public class IKAnalzyerTest {

    @Test
    public void testAnalyzer() {
        //构建IK分词器，使用smart分词模式
        Analyzer analyzer = new IKAnalyzer(true);

        //获取Lucene的TokenStream对象
        TokenStream ts = null;
        try {
            ts = analyzer.tokenStream("myfield",
                    new StringReader("这是一个中文分词的例子，你可以直接运行它！IKAnalyer can analysis english text too"));
            //获取词元位置属性
            OffsetAttribute offset = ts.addAttribute(OffsetAttribute.class);
            //获取词元文本属性
            CharTermAttribute term = ts.addAttribute(CharTermAttribute.class);
            //获取词元文本属性
            TypeAttribute type = ts.addAttribute(TypeAttribute.class);

            //重置TokenStream（重置StringReader）
            ts.reset();
            //迭代获取分词结果
            while (ts.incrementToken()) {
                System.out.println(
                        offset.startOffset() + " - " + offset.endOffset() + " : " + term.toString() + " | " + type
                                .type());
            }
            //关闭TokenStream（关闭StringReader）
            ts.end();   // Perform end-of-stream operations, e.g. set the final offset.

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            //释放TokenStream的所有资源
            if (ts != null) {
                try {
                    ts.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
    }
}
```

下面的代码测试索引和搜索，如代码清单 4.9.2 所示：

```java
public class LuceneIndexAndSearchTest {

    /**
     * 模拟：
     * 创建一个单条记录的索引，并对其进行搜索
     */
    @Test
    public void searchTest() throws IOException {
        //Lucene Document的域名
        String fieldName = "text";
        //检索内容
        String text = "IK Analyzer是一个结合词典分词和文法分词的中文分词开源工具包。它使用了全新的正向迭代最细粒度切分算法。";

        //实例化IKAnalyzer分词器
        Analyzer analyzer = new IKAnalyzer(true);

        Directory directory = null;
        IndexWriter iwriter = null;
        IndexReader ireader = null;
        IndexSearcher isearcher = null;
        try {
            //建立内存索引对象
            directory = new ByteBuffersDirectory();

            //配置IndexWriterConfig
            IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
            iwConfig.setOpenMode(OpenMode.CREATE_OR_APPEND);
            iwriter = new IndexWriter(directory, iwConfig);
            //写入索引
            Document doc = new Document();
            doc.add(new StringField("ID", "10000", Field.Store.YES));
            doc.add(new TextField(fieldName, text, Field.Store.YES));
            iwriter.addDocument(doc);
            iwriter.close();

            //搜索过程**********************************
            //实例化搜索器
            ireader = DirectoryReader.open(directory);
            isearcher = new IndexSearcher(ireader);

            String keyword = "中文分词工具包";
            //使用QueryParser查询分析器构造Query对象
            QueryParser qp = new QueryParser(fieldName, analyzer);
            qp.setDefaultOperator(QueryParser.AND_OPERATOR);
            Query query = qp.parse(keyword);
            System.out.println("Query = " + query);

            //搜索相似度最高的5条记录
            TopDocs topDocs = isearcher.search(query, 5);
            System.out.println("命中：" + topDocs.totalHits);
            //输出结果
            ScoreDoc[] scoreDocs = topDocs.scoreDocs;
            for (int i = 0; i < topDocs.totalHits.value; i++) {
                Document targetDoc = isearcher.doc(scoreDocs[i].doc);
                System.out.println("内容：" + targetDoc.toString());
            }

        } catch (CorruptIndexException e) {
            e.printStackTrace();
        } catch (LockObtainFailedException e) {
            e.printStackTrace();
        } catch (IOException e) {
            e.printStackTrace();
        } catch (ParseException e) {
            e.printStackTrace();
        } finally {
            if (ireader != null) {
                try {
                    ireader.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if (directory != null) {
                try {
                    directory.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
    }
}

```

除默认配置外，用户还可以通过 IKAnalyzer.cfg.xml 文件为 IKAnalyzer 配置用户词典和停用词列表，将配置文件与 IKAnalyzer.jar 放置同一目录下即可。配置文件格式如下：

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">  
<properties>  
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 
	<entry key="ext_dict">ext.dic;</entry> 
	-->
	<!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">stopword.dic;</entry> 
	
</properties>

要配置多个用户词典文件或停用词文件，使用分号（;）隔开。多行配置也可以。




<br/><br/>
<a id="2"></a>

## 4.9.2 中文分析器 Jcseg ##

Jcseg 是基于 mmseg 算法的一个轻量级中文分词器，同时集成了关键字提取，关键短语提取，关键句子提取和文章自动摘要等功能，并且提供了一个基于 Jetty 的 web 服务器，方便各大语言直接 http 调用，同时提供了最新版本的 lucene，solr 和elasticsearch 的分词接口！

源码：[https://github.com/lionsoul2014/jcseg](https://github.com/lionsoul2014/jcseg)

Jcseg 项目官网介绍比较详细，目前仍在更新。

Jcseg 自带了一个 jcseg.properties文件用于快速配置而得到适合不同场合的分词应用，例如：最大匹配词长，是否开启中文人名识别，是否追加拼音，是否追加同义词等！

Jcseg 核心功能：

>中文分词：mmseg算法 + Jcseg 独创的优化算法，四种切分模式。
关键字提取：基于textRank算法。
关键短语提取：基于textRank算法。
关键句子提取：基于textRank算法。
文章自动摘要：基于BM25+textRank算法。
自动词性标注：基于词库+（统计歧义去除计划），目前效果不是很理想，对词性标注结果要求较高的应用不建议使用。
命名实体标注：基于词库+（统计歧义去除计划），电子邮件，网址，大陆手机号码，地名，人名，货币，datetime时间，长度，面积，距离单位等。
Restful api：嵌入jetty提供了一个绝对高性能的server模块，包含全部功能的http接口，标准化json输出格式，方便各种语言客户端直接调用。

























