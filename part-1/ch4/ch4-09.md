## 4.9 中文分析 ##

上一节讨论了 Lucene 分析关于不同语言方面的支持，以及针对非英语语种进行分析时需要面对的问题，并对 Lucene 自带的三个有中文处理能力的的分析器进行了测试和比较。可以看出，这三个自带的分析器 StandardAnalyzer、CJKAnalyzer、SmartChineseAnalyzer 都有分析中文文本的能力，而且能力一个比一个强，以 SmartChineseAnalyzer 能力最为强大。但它们的处理效果还是差强人意，并不能满足生产级搜索程序的需求。因此，通常我们会采用第三方开发的针对中文语系的分析器来处理中文内容。

本节介绍几个知名的可用于生产级应用程序的第三方中文分析器，并对它们的特性和配置方法进行深入的探讨。

<br/><br/>
<a id="1"></a>

## 4.9.1 中文分析器 IKAnalyzer ##

IKAnalyzer 是业内知名且流行的开源中文分析器，作者林良益 [linliangyi2007@gmail.com](mailto:linliangyi2007@gmail.com) 最初 2006 年 12 月发布 1.0 版，并在 GoogleCode: [http://code.google.com/p/ik-analyzer](http://code.google.com/p/ik-analyzer) 上开源。至 2012 年，IKAnalyzer 推出了 4 个大版本。最初，它是以开源项目Luence为应用主体的，结合词典分词和文法分析算法的中文分词组件。从 3.0 版本开始，IKAnalyzer 发展为面向 Java 的公用分词组件，独立于 Lucene 项目，同时提供了对 Lucene 的默认优化实现。在 2012 版本中，IKAnalyzer 实现了简单的分词歧义排除算法，标志着 IKAnalyzer 分词器从单纯的词典分词向模拟语义分词衍化。


IKAnalyzer 2012 特性：
- 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和智能分词两种切分模式。
- 在系统环境：Core2 i7 3.4G双核，4G内存，window 7 64位， Sun JDK 1.6_29 64位 普通pc环境测试，IK2012具有160万字/秒（3000KB/S）的高速处理能力。
- 2012版本的智能分词模式支持简单的分词排歧义处理和数量词合并输出。
- 采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符。
- 优化的词典存储，更小的内存占用。支持用户词典扩展定义。特别的，在2012版本，词典支持中文，英文，数字混合词语。


2012 版之后，IKAnalyzer 已停止更新，github 上有很多克隆改造版本，用于适应新版本的 Lucene 以及 Solr 和 elasticsearch 使用环境。本小结内容克隆自 blueshen 的 github 版本 [https://github.com/blueshen/ik-analyzer.git](https://github.com/blueshen/ik-analyzer.git)，并为支持 Lucene 8.0 及其以上版本打了标签，方便使用。代码位于：

&emsp;&emsp;**[https://github.com/dearall/ik-analyzer.git](https://github.com/dearall/ik-analyzer.git)**

IKAnalyzer 有三个构造器：
- **IKAnalyzer()** 默认实现使用最细粒度切分算法
- **IKAnalyzer(boolean useSmart)** useSmart 为 true 时，分词器采用智能切分；为 false 时，分词器进行最细粒度切分。
- **IKAnalyzer(Configuration cfg)** 使用自定义的详细配置选项对分析器进行配置。

对应用开发者来说，使用 IKAnalyzer 最重要的就是对词典的配置。其中 Configuration 是分析器对外提供的配置选项接口，通过实现该接口，我们可以实现对 IKAnalyzer 的完全自定义配置。前两个方法使用是 IK 提供的默认配置选项实现 DefaultConfig 创建分析器。即便如此，DefaultConfig 也对外提供了可以扩展词典的配置文件 IKAnalyzer.cfg.xml。

Configuration 接口的定义如下：

```java

/**
 * 配置管理类接口
 */
public interface Configuration {

    /**
     * 返回useSmart标志位
     * useSmart =true ，分词器使用智能切分策略， =false则使用细粒度切分
     *
     * @return useSmart
     */
    boolean useSmart();

    /**
     * 设置useSmart标志位
     * useSmart =true ，分词器使用智能切分策略， =false则使用细粒度切分
     *
     * @param useSmart
     */
    void setUseSmart(boolean useSmart);

    /**
     * 获取主词典路径
     *
     * @return String 主词典路径
     */
    String getMainDictionary();

    /**
     * 获取量词词典路径
     *
     * @return String 量词词典路径
     */
    String getQuantifierDicionary();

    /**
     * 获取扩展字典配置路径
     *
     * @return List<String> 相对类加载器的路径
     */
    List<String> getExtDictionarys();

    /**
     * 获取扩展停止词典配置路径
     *
     * @return List<String> 相对类加载器的路径
     */
    List<String> getExtStopWordDictionarys();

}

```

该接口的定义比较简单，表示智能分析的 boolean 型开关 useSmart()。主词典路径 getMainDictionary() 和量词词典路径 getQuantifierDicionary()。扩展字典配置路径列表 getExtDictionarys() 和扩展停止词典配置路径列表 getExtStopWordDictionarys()。

&emsp;&emsp;IK 的默认配置选项实现 DefaultConfig 提供了三个默认配置的词典路径，它们都位于类路径的 "org/wltea/analyzer/dic/" 目录下，分别是主词典 main2012.dic，量词词典 quantifier.dic，停用词词典 stopword.dic。其中停用词词典 stopword.dic 中包含了中文和英文停用词列表，可以用于去除中文和英文中的停用词。主词典 main2012.dic 提供的词列表是非常有限的，因此 DefaultConfig 对外提供了可以扩展词典的配置文件 IKAnalyzer.cfg.xml。IKAnalyzer.cfg.xml 位于打包文件 IKAnalyzer.jar 目录之外，可以不用重新编译随意调整扩展词典的配置。配置文件格式如下：

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">  
<properties>  
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 
	<entry key="ext_dict">ext.dic;</entry> 
	-->
	<!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">stopword.dic;</entry> 
	
</properties>
```

要配置多个用户词典文件或停用词文件，使用分号（;）隔开配置。注意，IKAnalyzer.cfg.xml 使用 ` <!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">` 声明，通过 java 的 Properties 类读取，因此不能通过多行的 `<entry key="ext_dict">` 配置多个扩展词典，只能使用分号隔开的方式配置多个词典。

因此，无需自定义实现 Configuration 接口，也可以控制 IKAnalyzer 的使用。

词典内容是通过 Dictionary 单例类的 initial(Configuration cfg) 方法载入内存的。之后所有的分析过程都是通过 Dictionary 对象内部的三个词典数据结构类型 DictSegment 实例完成的：
- **DictSegment mainDict** 主词词典
- **DictSegment stopWordDict** 停用词词典
- **DictSegment quantifierDict** 量词词典

Dictionary 对外提供如下方法：
- **isStopWord(char[] charArray, int begin, int length)** 判断是否为停用词
- **matchInMainDict(char[] charArray)** 检索匹配主词典
- **matchInMainDict(char[] charArray, int begin, int length)** 检索匹配主词典
- **matchInQuantifierDict(char[] charArray, int begin, int length)** 检索匹配量词词典
- **matchWithHit(char[] charArray, int currentIndex, Hit matchedHit)** 从已匹配的Hit中直接取出DictSegment，继续向下匹配


<br/><br/>
下面看看如何使用 IKAnalyzer 分析器。IKAnalyzer 自带测试代码如代码清单 4.9.1 所示：

<table width="100%"><tr><td bgcolor=green><font color=black>Listing 4.9.1 IKAnalzyerTest</td></tr></table>

```java

/**
 * 使用IKAnalyzer进行分词的演示
 * 2012-10-22
 */
public class IKAnalzyerTest {

    @Test
    public void testAnalyzer() {
        //构建IK分词器，使用smart分词模式
        Analyzer analyzer = new IKAnalyzer(true);

        //获取Lucene的TokenStream对象
        TokenStream ts = null;
        try {
            ts = analyzer.tokenStream("myfield",
                    new StringReader("这是一个中文分词的例子，你可以直接运行它！IKAnalyer can analysis english text too. This is an test " +
                            "case."));
            //获取词元位置属性
            OffsetAttribute offset = ts.addAttribute(OffsetAttribute.class);
            //获取词元文本属性
            CharTermAttribute term = ts.addAttribute(CharTermAttribute.class);
            //获取词元文本属性
            TypeAttribute type = ts.addAttribute(TypeAttribute.class);

            //重置TokenStream（重置StringReader）
            ts.reset();
            //迭代获取分词结果
            while (ts.incrementToken()) {
                System.out.println(
                        offset.startOffset() + " - " + offset.endOffset() + " : " + term.toString() + " | " + type
                                .type());
                System.out.println("token: " + ts.reflectAsString(false));
            }
            //关闭TokenStream（关闭StringReader）
            ts.end();   // Perform end-of-stream operations, e.g. set the final offset.

        } catch (IOException e) {
            e.printStackTrace();
        } finally {
            //释放TokenStream的所有资源
            if (ts != null) {
                try {
                    ts.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
    }
}
```

代码测试通过，并输出：

```shell
13:41:08.610 [main] INFO org.wltea.analyzer.dic.Dictionary - 加载扩展词典:ext.dic
13:41:08.620 [main] INFO org.wltea.analyzer.dic.Dictionary - 加载扩展停止词典:org/wltea/analyzer/dic/stopword.dic
13:41:08.620 [main] INFO org.wltea.analyzer.dic.Dictionary - 加载扩展停止词典:stopword.dic
0 - 2 : 这是 | CN_WORD
token: term=这是,bytes=[e8 bf 99 e6 98 af],startOffset=0,endOffset=2,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1
2 - 4 : 一个 | CN_WORD
token: term=一个,bytes=[e4 b8 80 e4 b8 aa],startOffset=2,endOffset=4,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1
4 - 6 : 中文 | CN_WORD
token: term=中文,bytes=[e4 b8 ad e6 96 87],startOffset=4,endOffset=6,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1
6 - 8 : 分词 | CN_WORD
token: term=分词,bytes=[e5 88 86 e8 af 8d],startOffset=6,endOffset=8,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1
9 - 11 : 例子 | CN_WORD
token: term=例子,bytes=[e4 be 8b e5 ad 90],startOffset=9,endOffset=11,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1
15 - 17 : 直接 | CN_WORD
token: term=直接,bytes=[e7 9b b4 e6 8e a5],startOffset=15,endOffset=17,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1
17 - 19 : 运行 | CN_WORD
token: term=运行,bytes=[e8 bf 90 e8 a1 8c],startOffset=17,endOffset=19,positionIncrement=1,positionLength=1,type=CN_WORD,termFrequency=1
21 - 30 : ikanalyer | ENGLISH
token: term=ikanalyer,bytes=[69 6b 61 6e 61 6c 79 65 72],startOffset=21,endOffset=30,positionIncrement=1,positionLength=1,type=ENGLISH,termFrequency=1
31 - 34 : can | ENGLISH
token: term=can,bytes=[63 61 6e],startOffset=31,endOffset=34,positionIncrement=1,positionLength=1,type=ENGLISH,termFrequency=1
35 - 43 : analysis | ENGLISH
token: term=analysis,bytes=[61 6e 61 6c 79 73 69 73],startOffset=35,endOffset=43,positionIncrement=1,positionLength=1,type=ENGLISH,termFrequency=1
44 - 51 : english | ENGLISH
token: term=english,bytes=[65 6e 67 6c 69 73 68],startOffset=44,endOffset=51,positionIncrement=1,positionLength=1,type=ENGLISH,termFrequency=1
52 - 56 : text | ENGLISH
token: term=text,bytes=[74 65 78 74],startOffset=52,endOffset=56,positionIncrement=1,positionLength=1,type=ENGLISH,termFrequency=1
57 - 61 : too. | LETTER
token: term=too.,bytes=[74 6f 6f 2e],startOffset=57,endOffset=61,positionIncrement=1,positionLength=1,type=LETTER,termFrequency=1
73 - 77 : test | ENGLISH
token: term=test,bytes=[74 65 73 74],startOffset=73,endOffset=77,positionIncrement=1,positionLength=1,type=ENGLISH,termFrequency=1
78 - 83 : case. | LETTER
token: term=case.,bytes=[63 61 73 65 2e],startOffset=78,endOffset=83,positionIncrement=1,positionLength=1,type=LETTER,termFrequency=1

Process finished with exit code 0
```

&emsp;&emsp;IKAnalyzer 通过单一的 IKTokenizer 实现，其后没有链接任何词元过滤器。IKTokenizer 只实现了词元的 **CharTermAttribute、OffsetAttribute、TypeAttribute** 三个有效的词元特性，没有实现其它特性，使用时需要注意。从上例代码输出可以看出，即便移除了停用词，后面词元的位置增量也没有变化，都是 1。因此，**IKAnalyzer 的分析结果不能直接适用与位置相关的查询类型，如短语查询 PhraseQuery、跨度查询 SpanQuery 等**。要使 IKAnalyzer 适用与位置相关的查询，需要修改 IKTokenizer 和 IKAnalyzer 的实现，使其计算出正确的位置增量，有可能还要修改其它的地方，比如 AnalyzeContext 类的 `Lexeme getNextLexeme()` 方法也要修改。其实，可以将 IKTokenizer 类拆分为 IKTokenizer 和 IKStopFilter 实现，然后将 IKTokenizer 和 IKStopFilter 链接起来实现 IKAnalyzer。这里不再展开，有需要的时候再自行实现。


下面的代码测试索引和搜索，如代码清单 4.9.2 所示：

<table width="100%"><tr><td bgcolor=green><font color=black>Listing 4.9.2 LuceneIndexAndSearchTest</td></tr></table>

```java
public class LuceneIndexAndSearchTest {

    /**
     * 模拟：
     * 创建一个单条记录的索引，并对其进行搜索
     */
    @Test
    public void searchTest() throws IOException {
        //Lucene Document的域名
        String fieldName = "text";
        //检索内容
        String text = "IK Analyzer是一个结合词典分词和文法分词的中文分词开源工具包。它使用了全新的正向迭代最细粒度切分算法。";

        //实例化IKAnalyzer分词器
        Analyzer analyzer = new IKAnalyzer(true);

        Directory directory = null;
        IndexWriter iwriter = null;
        IndexReader ireader = null;
        IndexSearcher isearcher = null;
        try {
            //建立内存索引对象
            directory = new ByteBuffersDirectory();

            //配置IndexWriterConfig
            IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer);
            iwConfig.setOpenMode(OpenMode.CREATE_OR_APPEND);
            iwriter = new IndexWriter(directory, iwConfig);
            //写入索引
            Document doc = new Document();
            doc.add(new StringField("ID", "10000", Field.Store.YES));
            doc.add(new TextField(fieldName, text, Field.Store.YES));
            iwriter.addDocument(doc);
            iwriter.close();

            //搜索过程**********************************
            //实例化搜索器
            ireader = DirectoryReader.open(directory);
            isearcher = new IndexSearcher(ireader);

            String keyword = "中文分词工具包";
            //使用QueryParser查询分析器构造Query对象
            QueryParser qp = new QueryParser(fieldName, analyzer);
            qp.setDefaultOperator(QueryParser.AND_OPERATOR);
            Query query = qp.parse(keyword);
            System.out.println("Query = " + query);

            //搜索相似度最高的5条记录
            TopDocs topDocs = isearcher.search(query, 5);
            System.out.println("命中：" + topDocs.totalHits);
            //输出结果
            ScoreDoc[] scoreDocs = topDocs.scoreDocs;
            for (int i = 0; i < topDocs.totalHits.value; i++) {
                Document targetDoc = isearcher.doc(scoreDocs[i].doc);
                System.out.println("内容：" + targetDoc.toString());
            }

        } catch (CorruptIndexException e) {
            e.printStackTrace();
        } catch (LockObtainFailedException e) {
            e.printStackTrace();
        } catch (IOException e) {
            e.printStackTrace();
        } catch (ParseException e) {
            e.printStackTrace();
        } finally {
            if (ireader != null) {
                try {
                    ireader.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
            if (directory != null) {
                try {
                    directory.close();
                } catch (IOException e) {
                    e.printStackTrace();
                }
            }
        }
    }
}

```

执行程序，输出：

```shell
15:55:46.847 [main] INFO org.wltea.analyzer.dic.Dictionary - 加载扩展词典:ext.dic
15:55:46.867 [main] INFO org.wltea.analyzer.dic.Dictionary - 加载扩展停止词典:org/wltea/analyzer/dic/stopword.dic
15:55:46.887 [main] INFO org.wltea.analyzer.dic.Dictionary - 加载扩展停止词典:stopword.dic
Query = +text:中文 +text:分词 +text:工具包
命中：1 hits
内容：Document<stored,indexed,tokenized,omitNorms,indexOptions=DOCS<ID:10000> stored,indexed,tokenized<text:IK Analyzer是一个结合词典分词和文法分词的中文分词开源工具包。它使用了全新的正向迭代最细粒度切分算法。>>

Process finished with exit code 0
```



<br/><br/>
<a id="2"></a>

## 4.9.2 中文分析器 Jcseg ##

Jcseg 是基于 mmseg 算法的一个轻量级中文分词器，同时集成了关键字提取，关键短语提取，关键句子提取和文章自动摘要等功能，并且提供了一个基于 Jetty 的 web 服务器，方便各大语言直接 http 调用，同时提供了最新版本的 lucene，solr 和elasticsearch 的分词接口！

源码：[https://github.com/lionsoul2014/jcseg](https://github.com/lionsoul2014/jcseg)

Jcseg 项目官网介绍比较详细，目前仍在更新。

Jcseg 自带了一个 jcseg.properties文件用于快速配置而得到适合不同场合的分词应用，例如：最大匹配词长，是否开启中文人名识别，是否追加拼音，是否追加同义词等！

Jcseg 核心功能：

>中文分词：mmseg算法 + Jcseg 独创的优化算法，四种切分模式。
关键字提取：基于textRank算法。
关键短语提取：基于textRank算法。
关键句子提取：基于textRank算法。
文章自动摘要：基于BM25+textRank算法。
自动词性标注：基于词库+（统计歧义去除计划），目前效果不是很理想，对词性标注结果要求较高的应用不建议使用。
命名实体标注：基于词库+（统计歧义去除计划），电子邮件，网址，大陆手机号码，地名，人名，货币，datetime时间，长度，面积，距离单位等。
Restful api：嵌入jetty提供了一个绝对高性能的server模块，包含全部功能的http接口，标准化json输出格式，方便各种语言客户端直接调用。

























