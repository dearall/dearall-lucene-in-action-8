## 4.3 使用内置分析器 Using the built-in analyzers ##

Lucene 自带了几个分析器，通过将某些内置的词元分词器 Tokenizer 和词元过滤器 TokenFilter 链接在一起创建。表 4.3 列出几个主要的分析器。

<br/>
<div align=center>表 4.3 Lucene 主要的内置分析器</div>

<table>
    <tr bgcolor=#AA0000>
        <th align=center>分析器 Analyzer</th>
        <th align=center>内部执行步骤</th>
    </tr>
    <tr>
      <td>WhitespaceAnalyzer</td>
      <td>以空白字符作为分割条件，将原始文本切分为词元。实际利用 Character.isWhitespace(int codePoint) 方法作为判断依据，</td>
    </tr>
    <tr>
      <td>SimpleAnalyzer</td>
      <td>以非字母字符作为分割条件，将原始文本切分为词元，并将词元转换为小写形式</td>
    </tr>
    <tr>
      <td>StopAnalyzer</td>
      <td>以非字母字符作为分割条件，将原始文本切分为词元，并将词元转换为小写形式，然后移除停用词</td>
    </tr>
    <tr>
      <td>KeywordAnalyzer</td>
      <td>将整个文本作为一个单独的词元</td>
    </tr>
    <tr>
      <td>StandardAnalyzer</td>
      <td>基于复杂的语法对原始文本进行分词，可以识别出 email 地址，首字母缩略词，中文-日文-韩文字符，字母数字，等等。它也转换为小写形式，并移除停用词</td>
    </tr>
</table>

这几个内置的分析器：WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, KeywordAnalyzer, 以及 StandardAnalyzer 是设计用于分析几乎所有的西方语系（基于欧洲的）文本。


<br/><br/>
<a id="1"></a>
## 4.3.1 StopAnalyzer ##

StopAnalyzer 除了基本的单词切分和将词元转换为小写形式，还移除特定的单词，称为“**停用词（stop words）**”。停用词是使用非常频繁的普通单词，例如 the, a, an, and 等，它们对于搜索来说没有什么独立的词义，几乎所有的文档中都包含这些词汇。

&emsp;&emsp;在 analyzers-common 模块中有一个 EnglishAnalyzer 分析器，其中定义了一个静态的 ENGLISH_STOP_WORDS_SET 字段，该字段定义了英语语系中一些常用的停用词列表，StopAnalyzer 可以使用该集合进行构建。ENGLISH_STOP_WORDS_SET 定义的停用词列表如下所示：

```
    "a", "an", "and", "are", "as", "at", "be", "but", "by",
    "for", "if", "in", "into", "is", "it",
    "no", "not", "of", "on", "or", "such",
    "that", "the", "their", "then", "there", "these",
    "they", "this", "to", "was", "will", "with"
```

StopAnalyzer 提供了如下构造器用于创建实例：

- **StopAnalyzer(CharArraySet stopWords)**
- **StopAnalyzer(Path stopwordsFile)** 
- **StopAnalyzer(Reader stopwords)**

其中构造器的参数，使用不同类型提供的停用词列表，如上面提到的 EnglishAnalyzer.ENGLISH_STOP_WORDS_SET 即可作为 StopAnalyzer(CharArraySet stopwords) 构造器参数创建实例。


<br/><br/>
<a id="2"></a>
## 4.3.2 StandardAnalyzer ##

StandardAnalyzer 是公认的最使用的 Lucene 内置分析器。基于 JFlex 语法分析，使用如下词汇类型进行词元分词：alphanumerics（字母数字）、acronyms（首字母缩略词）、公司名称、email 地址、计算机主机名、数字、内部带有 ' 省略词的单词、序列号、IP 地址、以及中文、日文、韩文字符。StandardAnalyzer 内置停用词过滤，机制与 StopAnalyzer 分析器相同，因此使用 StandardAnalyzer 是最明智的选择。

StandardAnalyzer 提供如下构造器：

- **StandardAnalyzer()** 默认构造器没有提供停用词列表，StandardAnalyzer 内部使用一个空的列表，因此不移除停用词。
- **StandardAnalyzer(CharArraySet stopWords)** 使用 CharArraySet 类型的停用词列表，可以 EnglishAnalyzer.ENGLISH_STOP_WORDS_SET。
- **StandardAnalyzer(Reader stopwords)** 使用 Reader 类型的停用词列表。


使用 StandardAnalyzer 与使用其它分析器没什么不同，如代码清单 4.2.1 AnalyzerDemo 类所示。然而，它的特别之处在于其处理效果，最明显的是对文本进行不同的处理。例如，4.1 节比较不同的分析器对短语 "XY&Z Corporation - xyz@example.com" 不同的分析结果。StandardAnalyzer 是唯一把 'XY&Z' 保留在一起作为词元的分析器，同理，email 地址 'xyz@example.com' 也是作为一个词元保留的，这两个实例都展示了 StandardAnalyzer 极为复杂的分析处理过程。


<br/><br/>
<a id="3"></a>
## 4.3.3 应该采用哪个核心分析器 ##

我们已经了解到每种核心分析器工作明显的区别，那么该如何为自己的应用程序选择合适的分析器呢？答案可能令人吃惊：大多数应用程序都不适应任意一种内置的分析器，而是选择创建自己的分析器链。对于使用某个个核心分析器的应用程序，StandardAnalyzer 可能是最常见的选择。其它的核心分析器通常对于大多数应用程序来说都太简单了，或许除了某些特殊的用例，比如，一个含有部分数字的域，可以使用 WhitespaceAnalyzer 将数字去掉。但这些核心分析器对测试用例非常合适，并且实际上 Lucene 的单元测试大量使用了这些分析器。

&emsp;&emsp;通常，应用程序有自己的特殊需求，例如自定义应用词列表，按应用程序特定的词元执行分词，例如部分数字或同义词扩大，对特定的词元保留大小写形式，或者选择特殊的还原词干算法等。实际上，Solr 可以通过在 solrconfig.xml 文件中以 XML 格式直接定义分析链，这使创建自己的分析链过程变得简单轻松。

&emsp;&emsp;记住这一点，现在有了 Lucene 分析过程的强大的基础知识，应该继续前行，去创建自己现实世界的分析器了。下面将展示如何实现一系列常用特性：近音查询和同义词扩充。之后，将创建自己的分析器链，并通过还原词干来规范化词元，移除停用词，以及讨论一些处理方式带来的的挑战。之后，将讨论一些影响分析的域特定的变体。最后，将探讨一些在分析不同语系时遇到的问题，并快速尝试一下 Nutch 项目是如何处理文档分析的。











