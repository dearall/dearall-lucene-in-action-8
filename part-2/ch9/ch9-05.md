## 9.5 评估搜索质量 Evaluating search quality ##

该如何测试自己搜索程序的相关性或者质量呢？相关性测试是至关重要的，因为，在一天结束的时候，如果用户没有获得相关结果，它们是不会满意的。对使用 Lucene 很多小的改变，从分析器链，到索引哪些域，到如何构建出 Query，到如何自定义评分，都可能对相关性造成巨大的影响。要是能正确衡量出这些影响，那么就可以改进来提升应用程序的相关性。

&emsp;&emsp;然而，尽管搜索质量是搜索程序最重要的方面，但它却非常难以查明真相并对其施以控制。当然有一些主观的方法可以使用。可以运行一个可控的用户预测版本，或者自己试用自己的程序。能看出什么呢？除了检查出返回的文档是否相关之外，还有很多其他的事情可以检查：返回的搜索摘要准确吗？是否展示出正确的元数据？UI 界面能快速瞥一眼就能容易使用吗？难怪很少有搜索程序为其相关性进行调优。

&emsp;&emsp;那就是说，如果想要对返回文档的相关性进行客观地评测，所幸的是，benchmark 模块的 quality 包可以完成这个工作。这些类提供的具体实现是基于从 TREC 文集格式的，但也可以实现自己的格式。需要一个 “ground truth” 转录的查询集，其中每个查询列出与之相关的文档。这个方法我完全二元的：索引中一个给定的文档，被认为要么是相关的，要么是不相关的。有了这些，就可以计算精确度（precision）和召回（recall）值，这是信息检索社区对客观衡量搜索结果相关度的标准指标。精确度衡量对每个查询返回的文档哪些子集是相关的。例如，例如，如果一个查询有 20 个命中结果，但只有一个是相关的，那么精确度就是 0.05。如果只有一个命中结果，并且它是相关的，那么精确度是 1.0。召回 recall 衡量的是查询返回的相关文档所占的百分数。例如，如果查询列出 8 个具有相关性的文档，但在结果集中返回了 6 个，那么 recall 的值为 0.75。

&emsp;&emsp;在一个正确配置的搜索程序中，这两个指标很自然地相互对立。比方说，在一种极端的情况下，只向用户展示匹配查询得分最高的文档（top 1）。利用这个方法，这个精确度通常很高，因为第一个返回结果最有可能相关，而召回 recall 的值则会非常低，因为如果对这个查询有很多的相关文档，但只返回了其中的一个。如果将 top 1 增加到 top 10，那么每个查询会立刻返回很多的文档。有必要把精确度降下来，因为很可能现在允许一些不相关的文档返回到结果集中。但召回 recall 应有所提高，因为每次查询应该返回其相关文档的更大子集。

&emsp;&emsp;还有，想要相关文档在排名（ranking）列表中有更高的位置。考虑到这一点，计算一个平均精确度。这个指标在每一个 N 参数（cutoff）上计算精确度，N 的范围从 1 到最大值，然后计算出其平均值。因此如果搜索程序通常返回相关文档是在结果集靠前的位置，这个指标会更高。中数平均精确度（Mean average precision，MAP）会计算跨一个查询集的中数平均精确度。相关的指标，中数倒数排名（mean reciprocal rank，MRR），通过 1/M 计算，其中 M 表示第一个相关文档的排名。我们的目标是要让这两个数值越大越好。

代码清单 9.5 展示如何利用 quality 包计算精确度（precision）和召回（recall）值。目前，要衡量搜索质量，必须编写自己的 Java 代码（没有内置的任务来执行这个功能，可以仅使用一个算法脚本来执行计算工作）。要测试的查询表示为 QualityQuery 实例的数组。TrecTopicsReader 知道如何将 TREC 主题（topic）格式文件读取到 QualityQuery 实例中，但也可以实现自己的自定义设计。下一步，ground truth 通过简单的 Judge 接口表示。TrecJudge 类加载 TREC 的 Qrel 格式文件并实现 Judge。QualityQueryParser 将每个 QualityQuery 解析成真正的 Lucene 查询 Query。最后，QualityBenchmark 通过在所提供的 IndexSearcher 上执行搜索来测试查询。它返回一个 QualityStats 数组，每一个元素对应一个查询。QualityStats.average() 方法计算并报告精确度（precision）和召回（recall）值。

<table width="100%"><tr><td bgcolor=green><font color=black>Listing 9.5 对 IndexSearcher 计算 精确度（precision）和召回（recall）值</td></tr></table>


```java
public class PrecisionRecall {

  public static void main(String[] args) throws Throwable {

    File topicsFile = new File("benchmark/topics.txt");
    File qrelsFile = new File("benchmark/qrels.txt");

    Directory dir = FSDirectory.open(Paths.get("meetlucene/indexes/MeetLucene"));
    DirectoryReader reader = DirectoryReader.open(dir);
    IndexSearcher searcher = new IndexSearcher(reader);

    String docNameField = "filename"; 
    
    PrintWriter logger = new PrintWriter(System.out, true); 

    TrecTopicsReader qReader = new TrecTopicsReader();   //①
    QualityQuery qqs[] = qReader.readQueries(            //①
        new BufferedReader(new FileReader(topicsFile))); //①
    
    Judge judge = new TrecJudge(new BufferedReader(      //②
        new FileReader(qrelsFile)));                     //②
    
    judge.validateData(qqs, logger);                     //③
    
    QualityQueryParser qqParser = new SimpleQQParser("title", "contents");  //④
    
    QualityBenchmark qrun = new QualityBenchmark(qqs, qqParser, searcher, docNameField);
    SubmissionReport submitLog = null;
    QualityStats stats[] = qrun.execute(judge,           //⑤
            submitLog, logger);
    
    QualityStats avg = QualityStats.average(stats);      //⑥
    avg.log("SUMMARY",2,logger, "  ");//⑥

    reader.close();
    dir.close();
  }
}

```

① 读取 TREC topics 文件创建 QualityQuery[] 数组
② 使用 TrecJudge 加载 TREC 的 Qrel 格式文件，创建 Judge 实例
③ 验证查询与 Judge 匹配
④ 创建 QualityQueryParser 用于将质量查询解析为真正的 Lucene 查询
⑤ 执行基线测试 benchmark
⑥ 打印精确度（precision）和召回（recall）值指标


运行代码，获得如下输出：

```shell

SUMMARY
  Search Seconds:         0.043
  DocName Seconds:        0.021
  Num Points:            15.000
  Num Good Points:        3.000
  Max Good Points:        3.000
  Average Precision:      1.000
  MRR:                    1.000
  Recall:                 1.000
  Precision At 1:         1.000
  Precision At 2:         1.000
  Precision At 3:         1.000
  Precision At 4:         0.750
  Precision At 5:         0.600
  Precision At 6:         0.500
  Precision At 7:         0.429
  Precision At 8:         0.375
  Precision At 9:         0.333
  Precision At 10:        0.300
  Precision At 11:        0.273
  Precision At 12:        0.250
  Precision At 13:        0.231
  Precision At 14:        0.214

```

这个测试利用了 MeetLucene 模块创建的索引。这是个没有什么价值的测试，因为运行在一个单个的查询上，正好三个正确的文档（查看源代码目录下 benchmark/topics.txt 文件对应查，benchmark/qrels.txt 文件对应三个正确的文档）。可以看到，对三个 top 3 结果，精确度是最好的，值为 1.0，含义是这 top 3 个结果事实上是该条查询对应的正确结果。除了这 top 3 的结果精确度变差，因为其它文档是不正确的。召回 recall 的值为最好的 1.0，因为全部的 3 个文档都返回了。在实际的测试中，是不会看到这么完美的数值的。





